{
  "name" : "Convexification",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Convexifying Matrix Factorization",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Matrix Factorization as a Rank-Constrained Problem\nMatrix \\\\( Y\\in\\Re^{n,d} \\\\). For any \\\\( K \\in \\mathbb{N} \\\\),\n\n- \\\\( \\min_{\\mathbf{U}\\in\\Re^{N\\times K},\\mathbf{V}\\in\\Re^{M\\times K}} \\||_F \\mathbf{Y} - \\mathbf{U} \\mathbf{V}^T \\|| \\\\)\n\nIs equivalent to:\n\n- \\\\( \\min_{\\mathbf{X}\\in\\Re^{N\\times M},\\mathtt{rank}(\\mathbf{X})\\le K} \\||_F \\mathbf{Y} - \\mathbf{X} \\|| \\\\)\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "History",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "History of Convex Factorization\n\n* 2001: Fazel, Maryam, Haitham Hindi, Stephen P Boyd. \"A rank minimization heuristic with application to minimum order system approximation.\n* 2009: Cand√®s, Emmanuel J, and Benjamin Recht. \"Exact matrix completion via convex optimization.\".\n* 2009: Wright, John, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. \"Robust principal component analysis\".\n* 2011: Tamioka et Suzu. Statistical performance of convex tensor decomposition.\n* 2013: Bouchard et al. Convex Collective Matrix Factorization.\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Consequences\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### What Do We Gain By Making the Problem Convex?\nWe open the Pandor box of convex optimization tools!\n\nTheoretical guarantees\n\n- Convergence to the global optimum, no need for smart initialization\n- Speed of convergences\n- Polynomial time guarantees for a fixed accuracy \n\nBut more importantly, new algorithms:\n\n- Proximal methods (ISTA and FISTA)\n- Frank-Wolfe (Conjugate Gradient) algorithms\n- Augmented Lagrangian approaches (ADMM, splitting methods)\n- Stochastic variants\n",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "CombiningRegularization",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Combining Regularizations\n\nConvex penalties can be added to get better models\n\n- Sparse plus low-rank\n\n    \\\\(L_1\\\\) penalty added to the trace-norm\n  \n* Convex Cannonical Correlation Analysis\n    \n    Sum of trace norms\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
