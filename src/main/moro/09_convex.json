{
  "name" : "Convexification",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Convexifying Matrix Factorization",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Matrix Factorization as a Rank-Constrained Problem\nMatrix \\\\( Y\\in\\Re^{n,d} \\\\). For any \\\\( K \\in \\mathbb{N} \\\\),\n\n(1) \\\\( \\min_{\\mathbf{U}\\in\\Re^{N\\times K},\\mathbf{V}\\in\\Re^{M\\times K}} \\|| \\mathbf{Y} - \\mathbf{U} \\mathbf{V}^T \\||_F \\\\)\n\nIs equivalent to:\n\n(2) \\\\( \\min_{\\mathbf{X}\\in\\Re^{N\\times M},\\mathtt{rank}(\\mathbf{X})\\le K} \\|| \\mathbf{Y} - \\mathbf{X} \\||_F \\\\)\n\nWhich is relaxed into:\n\n(3) \\\\( \\min_{\\mathbf{X}\\in\\Re^{N\\times M},\\||\\mathbf{X}||_*\\le K} \\|| \\mathbf{Y} - \\mathbf{X} \\||_F \\\\)\n\n(3) is convex and solvable in polynomial time, while (1) and (2) are not.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "History",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "History of Convex Factorization\n\n* 2001: Fazel, Maryam, Haitham Hindi, Stephen P Boyd. \"A rank minimization heuristic with application to minimum order system approximation.\n* 2009: CandÃ¨s, Emmanuel J, and Benjamin Recht. \"Exact matrix completion via convex optimization.\".\n* 2009: Wright, John, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. \"Robust principal component analysis\".\n* 2011: Tamioka et Suzu. Statistical performance of convex tensor decomposition.\n* 2013: Bouchard et al. Convex Collective Matrix Factorization.\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Consequences\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### What Do We Gain By Making the Problem Convex?\nWe open the Pandor box of convex optimization tools!\n\nTheoretical guarantees\n\n- Convergence to the global optimum, no need for smart initialization\n- Speed of convergences\n- Polynomial time guarantees for a fixed accuracy \n\nBut more importantly, new algorithms:\n\n- Proximal methods (ISTA and FISTA)\n- Frank-Wolfe (Conjugate Gradient) algorithms\n- Augmented Lagrangian approaches (ADMM, splitting methods)\n- Stochastic variants\n",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "CombiningRegularization",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Combining Regularizations\n\nConvex penalties can be added to get better models\n\n- Sparse plus low-rank\n\n    \\\\(L_1\\\\) penalty added to the trace-norm\n  \n* Convex Cannonical Correlation Analysis\n    \n    Sum of trace norms\n\n    \\\\( \\min_{\\mathbf{S}+\\mathbf{L}} \\ell(S+L;Y) + \\lambda||S||_1 + \\mu||L||_* \n\\\\)\n\nVery popular in image processing ==> should also be useful in NLP!\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Spectral1",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Spectral Learning in NLP\n\nA promising application of convex factorization: **polynomial-time** learning of \n\n* HMMs\n* WFSAs\n* Grammars\n\n*Idea*: factorization of redundant moment matrices. \n\n**Example** \n    \n> **Empirical Hankel matrix of a string** \n\n> Matrix of counts of all possible prefixes and suffixes \n\n<img src=\"../../assets/figures/09/make_hankel.png\" width=\"500\">\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "conclusion",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Summary of Convex Factorization\n\n* **Trace norm** is great\n    * from intractable to tractable problems\n* **Spectral learning** combines well with trace norm\n    * Learning latent variable with computational guarantees\n    \n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
