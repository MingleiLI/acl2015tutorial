{
  "name" : "Non-Negative Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "why",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Non-negativity\n<br>\n\nLow-rank factors are commonly interpreted as:\n\n- document-topic, topic-term loadings\n\n- face-feature, feature-basis image\n\n- ...\n\n> MF is commomly used to find how each instance is composed of its parts; <br/> Subtraction is often counter-intuitive",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "definition",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<h3>Definition</h3>\n\n<br>\n<div style=\"text-align:center;\">\nGiven \\(\\mathbf{Y} \\in\\Re^{N\\times M}\\) <font color=\"blue\">\\(\\geq 0 \\)</font> <br>\n<br>\nfind \\(\\mathbf{U} \\in\\Re^{N\\times L}\\) <font color=\"blue\">\\(\\geq 0\\)</font> &nbsp; and &nbsp; \\(\\mathbf{V} \\in\\Re^{M\\times L}\\) <font color=\"blue\"> \\(\\geq 0\\)</font> <br>\n<br>\nso that \\(\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T}\\)\n</div>\n<br/>",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "> NMF is an additive mixture (admixture) model",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "algos",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Basic NMF algorithm\n\nMultiplicative update rules to minimize \\\\(||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}||^2 _2\\\\):\n\n$$u_{n,l} = u_{n,l} \\frac{(\\mathbf{Y}\\mathbf{V})_{n,l}}{(\\mathbf{Y}\\mathbf{Y}^{T}\\mathbf{U})_{n,l}} $$\n\n$$v_{m,l} = v_{m,l} \\frac{(\\mathbf{Y}^T\\mathbf{U})_{m,l}}{(\\mathbf{Y}^{T}\\mathbf{Y}\\mathbf{V})_{m,l}} $$\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br/>\nRelated to the additive SGD updates:\n\n$$u_{n,l} = u_{n,l} + \\eta ((\\mathbf{Y}\\mathbf{V})_{n,l} - (\\mathbf{Y}\\mathbf{Y}^{T}\\mathbf{U})_{n,l}) $$\n\n$$v_{m,l} = v_{m,l} + \\eta  ((\\mathbf{Y}^T\\mathbf{U})_{m,l}-(\\mathbf{Y}^{T}\\mathbf{Y}\\mathbf{V})_{m,l}) $$\n\n\n<div style=\"text-align:right;\">\n<i>Lee and Seung (2001)</i>\n</div>",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "prob",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### NMF with KL divergence\n<br>\n\nAssume that \\\\(\\mathbf{Y}\\\\) represents a probability distribution \n$$\\sum_{n,m} y_{n,m} = 1$$\n<br>\nMinimize the Kullback-Leibler divergence:\n\n$$ KL_{div}(\\mathbf{Y}||\\mathbf{U}\\mathbf{V}^{T}) = \\sum_{n,m}(y_{n,m}\\log\\frac{y_{n,m}}{(\\mathbf{U}\\mathbf{V}^{T})_{n,m}} )  $$\n\n- results in similar multiplicative updates\n- ensures that \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) also represent probability distributions\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "topic",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Probabilistic Latent Semantic Analysis/Indexing\n\n$$ y_{n,m} = P(n,m) = \\sum_{l} P(l) P(m|l) P(n|l) = u^\\prime_{n,l} v^{\\prime}_{l,m} $$\nwhere \\\\(n\\\\) is a document, \\\\(m\\\\) a word and \\\\(l\\\\) a component.",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br>\n$$\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T} = (\\mathbf{U}\\mathbf{A}^{-1}\\mathbf{A})(\\mathbf{V}\\mathbf{B}^{-1}\\mathbf{B})^T = (\\mathbf{U}\\mathbf{A}^{-1})(\\mathbf{A}\\mathbf{B})(\\mathbf{V}\\mathbf{B}^{-1})^T $$\n\nwhere \\\\(\\mathbf{A}=diag(\\sum_n v_{n,1},\\dots,\\sum_n v_{n,L})\\\\) <br> and \\\\(\\mathbf{B} = diag(\\sum_m u_{1,l}, \\dots, \\sum_m u_{m,L}) \\\\)\n\n",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br>\n\n> pLSA uses EM for parameter estimation and can converge to different local optima of the same objective as NMF-KL.\n<br>\n\n<div style=\"text-align:right;\">\n<i>Gaussier and Goutte (2005)</i>\n</div>",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "lda",
      "extraFields" : {
        "hide_output" : "false"
      }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Latent Dirichlet Allocation\n\n- For each topic \\\\(l\\\\)\n    - draw multinomial params over words \\\\(\\mathbf{v}_{:l}\\in\\Re^M\\\\) from \\\\(Dir(\\beta)\\\\)\n- For each document \\\\(n\\\\)\n    - draw multinomial params over topics \\\\(\\mathbf{u}_{n:}\\in\\Re^L\\\\) from \\\\(Dir(\\alpha)\\\\)\n    - for each word \\\\(i\\\\)\n        - sample a topic assignment \\\\(z_i \\\\) from \\\\(\\mathbf{u}_{n:}\\\\)\n        - sample a word \\\\(w_i \\\\) from \\\\(\\mathbf{v}_{:z_i}\\\\)\n<br>\n\nGiven the row-normalized document-term matrix \\\\(\\mathbf{Y}\\\\), find the factors \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) that reconstruct it. (Arora et al, 2012)\n\nCompared to standard NMF the challenge is that the samples forming the rows are very few and thus very noisy estimates.\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
