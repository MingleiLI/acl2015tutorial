{
  "name" : "Non-Negative Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "why",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Non-negativity\n\nIntepretability\n\n- zero has special meaning\n\nModelling assumptions\n\n- negative objects in images?\n- negative topics in documents?\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "definition",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Definition\n\nGiven \\\\(\\mathbf{Y} \\in\\Re^{N\\times M} \\geq 0\\\\)\n\nFind \\\\(\\mathbf{U} \\in\\Re^{N\\times L} \\geq 0 \\\\) and \\\\(\\mathbf{V} \\in\\Re^{M\\times L} \\geq 0\\\\)\n\nthat minimize the difference between \\\\(\\mathbf{Y}\\\\) and \\\\(\\mathbf{U}\\mathbf{V}^{T}\\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "algos",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Algorithm (Lee and Seung, 2001)\n\nMultiplicative update rules to minimize \\\\(||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}|| _2\\\\):\n\n\\\\(u_{n,l} = u_{n,l} \\frac{(\\mathbf{Y}\\mathbf{V})_{n,l}}{(\\mathbf{Y}\\mathbf{Y}^{T}\\mathbf{U})_{n,l}} \\\\)\n\n\\\\(v_{m,l} = v_{m,l} \\frac{(\\mathbf{Y}^T\\mathbf{U})_{m,l}}{(\\mathbf{Y}^{T}\\mathbf{Y}\\mathbf{V})_{m,l}} \\\\)\n\nRelated to the additive SGD updates\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "prob",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### NMF with KL divergence\n\nAssume that \\\\(\\mathbf{Y}\\\\) represents a probability distribution \\\\((\\sum_{n,m} y_{n,m} = 1)\\\\)\n\nMinimizing the Kullback-Leibler divergence:\n\n\\\\( KL_{div}(\\mathbf{Y}||\\mathbf{U}\\mathbf{V}^{T}) = \\sum_{n,m}(y_{n,m}\\log\\frac{y_{n,m}}{(\\mathbf{U}\\mathbf{V}^{T})_{n,m}} )  \\\\)\n\n- results in similar multiplicative updates\n- ensures that \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) also represent probability distributions\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "topic",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Probabilistic Latent Semantic Analysis/Indexing\n\nKL-NMF optimizes the same objective as pLSA/I (Gaussier and Goutte, 2005):\n\n\\\\(y_{n,m} = P(n,m) = \\sum_l P(l) P(m|l) P(n|l) = u^\\prime_{n,l} v^{\\prime}_{l,m} \\\\)\n\nwhere \\\\(n\\\\) is a document, \\\\(m\\\\) a word and \\\\(l\\\\) a component.\n\n \\\\( \\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T} = (\\mathbf{U}\\mathbf{A}^{-1}\\mathbf{A})(\\mathbf{V}\\mathbf{B}^{-1}\\mathbf{B})^T = (\\mathbf{U}\\mathbf{A}^{-1})(\\mathbf{A}\\mathbf{B})(\\mathbf{V}\\mathbf{B}^{-1})^T\\\\)\n\nwhere \\\\(\\mathbf{A}\\\\) and \\\\(\\mathbf{B} \\in\\Re^{L\\times L} \\\\) are identity matrices with \\\\(a_{l,l} = \\sum_n v_{n,l}\\\\) and \\\\(b_{l,l} = \\sum_m u_{m,l}\\\\)\n\npLSA uses Expectation-Maximization for parameter estimation and converges to different local optima.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "lda",
      "extraFields" : {
        "hide_output" : "false"
      }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Latent Dirichlet Allocation\nGenerative story:\n\n- For each topic \\\\(l\\\\)\n    - draw multinomial params over words \\\\(\\mathbf{v}_{:l}\\in\\Re^M\\\\) from \\\\(Dir(\\beta)\\\\)\n- For each document \\\\(n\\\\)\n    - draw multinomial params over topics \\\\(\\mathbf{u}_{n:}\\in\\Re^L\\\\) from \\\\(Dir(\\alpha)\\\\)\n    - for each word \\\\(i\\\\)\n        - sample a topic assignment \\\\(z_i \\\\) from \\\\(\\mathbf{u}_{n:}\\\\)\n        - sample a word \\\\(w_i \\\\) from \\\\(\\mathbf{v}_{:z_i}\\\\)\n\nGiven the row-normalized document-term matrix \\\\(\\mathbf{Y}\\\\), find the factors \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) that reconstruct it. (Arora et al, 2012)\n\nCompared to standard NMF the challenge is that the samples forming the rows, i.e. the words per document, are very few and thus very noisy estimates.\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
