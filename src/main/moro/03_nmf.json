{
  "name" : "Non-Negative Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "why",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Non-negativity\n\nLow-rank factors are commonly interpreted as:\n\n- topics\n- image features\n\n<img src=\"../../assets/figures/Faces.png\" height=\"200\">\n<div style=\"text-align:right;\"><font size=\"3\">\n<i>http://nimfa.biolab.si/nimfa.examples.orl_images.html</i></font>",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "> MF is commomly used to find how <br/> each instance is composed of its parts; <br/> Subtraction is often counter-intuitive",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 3,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "demoNMF",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "###Non-negative MF vs MF",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\nimport ml.wolfe.{Mat, Vect}\nimport ml.wolfe.util.Math._\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=100)\n",
      "extraFields" : {
        "hide_output" : "true",
        "showEditor" : "false",
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val n = 5\nval m = 5 \nval rand = new scala.util.Random(0)\nval rowNames = Seq(\"doc1\",\"doc2\",\"doc3\",\"doc4\",\"doc5\")\nval colNames = Seq(\"Greece\",\"Tsipras\",\"Germany\",\"crisis\",\"economy\")\nval names = header(rowNames,colNames)\nval rls = Matrix(hRulers = Seq(0,5), vRulers = Seq(0,5))\nval M = parseMatrix(\n    \"\"\"1 2 0 0 0\n       0 0 1 0 0\n       0 0 0 1 1 \n       0 0 1 1 1\n       1 2 1 1 0\"\"\")\n       \ndef optimizeL2(M: Mat, K: Int, iters: Int, alpha:Double = 0.1, initScale:Double = 1.0): (Seq[Vect], Seq[Vect]) = {\n  val rand = new scala.util.Random(0)\n  val AV = initialAV(K, M.dim1, M.dim2, initScale)\n  val A = AV._1; val V = AV._2\n  def update(i: Int, j: Int) = {\n    val a = A(i).copy\n    val v = V(j).copy\n    val y = a dot v\n    A(i) += v * alpha * (M(i, j) - y)\n    V(j) += a * alpha * (M(i, j) - y)\n  }\n  for (i <- Range(0,iters).toList) {\n    update(rand.nextInt(M.dim1), \n           rand.nextInt(M.dim2))\n  }\n  (A, V)\n}",
      "extraFields" : {
        "hide_output" : "true",
        "showEditor" : "false",
        "aggregatedCells" : "[\"import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\\nimport ml.wolfe.{Mat, Vect}\\nimport ml.wolfe.util.Math._\\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150)\"]"
      }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "\nval (_A,_V) = nmf(M, 2, 10)\nval m1 = opacity(matrix(M) + names,0,2).addRectClass(Set(4 -> 4),\"red-box\") + rls\nval m2 = opacity(matrix(dots(_A,_V)) + embeddings(_A,_V) + names,0,2) +rls\nval m3 = opacity(matrix(dots(_A,_V)),0,2) + numbers(embeddings(_A,_V)) + names +rls\nval m4 = numbers(matrix(dots(_A,_V)).addTextClass(Set(4 -> 4),\"red-text\") + names) +rls\nrender(Seq(m1,m2,m3,m4),layout)",
      "extraFields" : {
        "showEditor" : "false",
        "aggregatedCells" : "[\"import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\\nimport ml.wolfe.{Mat, Vect}\\nimport ml.wolfe.util.Math._\\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=100)\\n\",\"val n = 5\\nval m = 5 \\nval rand = new scala.util.Random(0)\\nval rowNames = Seq(\\\"doc1\\\",\\\"doc2\\\",\\\"doc3\\\",\\\"doc4\\\",\\\"doc5\\\")\\nval colNames = Seq(\\\"Greece\\\",\\\"Tsipras\\\",\\\"Germany\\\",\\\"crisis\\\",\\\"economy\\\")\\nval names = header(rowNames,colNames)\\nval rls = Matrix(hRulers = Seq(0,5), vRulers = Seq(0,5))\\nval M = parseMatrix(\\n    \\\"\\\"\\\"1 2 0 0 0\\n       0 0 1 0 0\\n       0 0 0 1 1 \\n       0 0 1 1 1\\n       1 2 1 1 0\\\"\\\"\\\")\\n       \\ndef optimizeL2(M: Mat, K: Int, iters: Int, alpha:Double = 0.1, initScale:Double = 1.0): (Seq[Vect], Seq[Vect]) = {\\n  val rand = new scala.util.Random(0)\\n  val AV = initialAV(K, M.dim1, M.dim2, initScale)\\n  val A = AV._1; val V = AV._2\\n  def update(i: Int, j: Int) = {\\n    val a = A(i).copy\\n    val v = V(j).copy\\n    val y = a dot v\\n    A(i) += v * alpha * (M(i, j) - y)\\n    V(j) += a * alpha * (M(i, j) - y)\\n  }\\n  for (i <- Range(0,iters).toList) {\\n    update(rand.nextInt(M.dim1), \\n           rand.nextInt(M.dim2))\\n  }\\n  (A, V)\\n}\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "definition",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Definition\n\n<img src=\"../../assets/figures/vanillaMF.png\" height=\"250\">\n\nGiven \\\\(\\mathbf{Y} \\in\\Re^{N\\times M}\\\\) <font color=\"blue\">\\\\(\\geq 0 \\\\)</font>\n\nfind \\\\(\\mathbf{U} \\in\\Re^{N\\times L}\\\\) <font color=\"blue\">\\\\(\\geq 0\\\\)</font> &nbsp; and &nbsp; \\\\(\\mathbf{V} \\in\\Re^{M\\times L}\\\\) <font color=\"blue\"> \\\\(\\geq 0\\\\)</font>\n\nso that \\\\(\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T}\\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n> NMF is additive mixture/(soft) clustering ",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "algos",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Basic NMF algorithm\n\nMultiplicative update rules to minimize \\\\(||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}||^2 _2\\\\):\n\n$$u_{n,l} = u_{n,l} \\frac{(\\mathbf{Y}\\mathbf{V})_{n,l}}{(\\mathbf{U}\\mathbf{V}^{T}\\mathbf{V})_{n,l}} $$ \n\n$$v_{m,l} = v_{m,l} \\frac{(\\mathbf{Y}^T\\mathbf{U})_{m,l}}{(\\mathbf{V}\\mathbf{U}^T\\mathbf{U})_{m,l}} $$\n\n<div style=\"text-align:right;\">\n<i>Lee and Seung (2001)</i>\n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n> Recent work have switched to constrained Alternating Least Squares optimization\n\n\n<div style=\"text-align:right;\">\n<i>Kim and Park (2008)</i>\n</div>\n",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "prob",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### NMF with KL divergence\n\nAssume that \\\\(\\mathbf{Y}\\\\) represents a probability distribution \n$$\\sum_{n,m} y_{n,m} = 1$$\n\nMinimize the Kullback-Leibler divergence:\n\n$$ KL_{div}(\\mathbf{Y}||\\mathbf{U}\\mathbf{V}^{T}) = \\sum_{n,m}(y_{n,m}\\log\\frac{y_{n,m}}{(\\mathbf{U}\\mathbf{V}^{T})_{n,m}} )  $$\n\n- results in similar multiplicative updates\n- ensures that \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) also represent probability distributions\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "topic",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Probabilistic Latent Semantic Analysis\n\n$$ y_{n,m} = P(n,m) \\\\\\\n= \\sum_{l} P(l) P(m|l) P(n|l)\n$$\n\nwhere \\\\(n\\\\) is a document, \\\\(m\\\\) a word and \\\\(l\\\\) a component.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n$$ \\dots  = u^\\prime_{n,l} v^{\\prime}_{l,m} $$\n\nwhere \\\\(u^\\prime_{n,l} = P(l) P(n|l) \\\\) and \\\\(v^{\\prime}_{l,m} = P(m|l) \\\\)\n",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 19,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "topic2",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Probabilistic Latent Semantic Analysis\n\n$$\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T} = (\\mathbf{U}\\mathbf{A}^{-1}\\mathbf{A})(\\mathbf{V}\\mathbf{B}^{-1}\\mathbf{B})^T\\\\\\ = (\\mathbf{U}\\mathbf{A}^{-1})(\\mathbf{A}\\mathbf{B})(\\mathbf{V}\\mathbf{B}^{-1})^T $$\n\n<div style=\"text-align:center;\">\nwhere \\(\\mathbf{A}=diag(\\sum_n v_{n,1},\\dots,\\sum_n v_{n,L}) \\)<br>\nand \\(\\mathbf{B}=diag(\\sum_m u_{m,1},\\dots,\\sum_m u_{m,L}) \\)\n</div>\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br/>\n> pLSA uses EM for parameter estimation and can converge to different local optima of the same objective as NMF-KL.\n\n<div style=\"text-align:right;\">\n<i>Gaussier and Goutte (2005)</i>\n</div> ",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "lda",
      "extraFields" : {
        "hide_output" : "false"
      }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Latent Dirichlet Allocation\n\n<img src=\"../../assets/figures/IntroToLDA.png\" height=\"350\">",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "ldamore",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Generative story\n\n- For each topic \\\\(l\\\\)\n    - draw multinomial over words \\\\(\\mathbf{v}_{:l}\\in\\Re^M\\\\) from \\\\(Dir(\\beta)\\\\)\n- For each document \\\\(n\\\\)\n    - draw multinomial over topics \\\\(\\mathbf{u}_{n:}\\in\\Re^L\\\\) from \\\\(Dir(\\alpha)\\\\)\n    - for each word \\\\(i\\\\)\n        - sample a topic assignment \\\\(z_i \\\\) from \\\\(\\mathbf{u}_{n:}\\\\)\n        - sample a word \\\\(w_i \\\\) from \\\\(\\mathbf{v}_{:z_i}\\\\)\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "> Given the row-normalized document-term matrix \\\\(\\mathbf{Y}\\\\), find the factors \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{V}\\\\) that reconstruct it.\n\n<div style=\"text-align:right;\">\n<i>Arora et al (2012)</i>\n</div>",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 27,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Summary",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "###Summary\n\n- Saw the matrix\n\n- Refresher on matrix rank and norms\n\n- Standard approaches to MF\n\n- Non-negative MF and connections to pLSA and LDA\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
