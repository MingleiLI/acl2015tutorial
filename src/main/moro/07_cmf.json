{
  "name" : "Collective Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Motivating_Example",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Arbitrary Database Factorization\n*Objective*: A probabilistic model for any database\n\n*Example*: Health data\n\n<br />\n<img src=\"../../assets/figures/06/relational_database_health.png\" width=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "tworel",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Modeling Two Relations\nAssume we have 2 relations:\n\n1. Document - Word matrix \\\\( Y^{(1)} \\in ( \\Re \\cup \\lbrace ? \\rbrace )^{n\\times F} \\\\)\n2. Document - Label matrix  \\\\( Y^{(2)} \\in \\lbrace 0,1,? \\rbrace ^{n\\times K} \\\\)\n\nRepresenting is a a 2-slices tensor is overly complicated and not needed.\nA simple and effective way to represent it is by concatenating the two matrices into a big \\\\( n\\times (F+K) \\\\) matrix:\n\n\\\\( Y := [ Y^{(1)} \\ Y^{(2)}]  \\\\)\n\nWe can estimate a probabilitic model on \\\\(Y\\\\) by factorizing it.\n\nJust use the binary matrix factorization technique we saw before. This is a new method for supervised classification!\n\n - Handles missing data in the input\n - Can work with partially labelled documents\n - Takes advantage of unlabelled data\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Motivating_cont",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<h3>Motivating Example</h3>\n<ul>\n<li>Document - Word matrix \\(Y^{(1)} \\in ( \\Re \\cup \\lbrace ? \\rbrace )^{N\\times F}\\)\n</li>\n<li>Document - Label matrix  \\(Y^{(2)} \\in \\lbrace 0,1,? \\rbrace ^{N\\times K}\\)\n</li>\n<li>\nWord - Label matrix \\( Y^{(3)} \\in \\lbrace 0, 1 ,? \\rbrace ^{K\\times F} \\)\n</li>\n</ul>\n\nNaive concatenation:\n\n$$\nY := \n\\left[\\begin{array}{cc} \nY^{(3)} & [?]_{K\\times K} \n\\\\\nY^{(1)} & Y^{(2)} \n\\end{array}\\right] \n$$\n\nWe can estimate a probabilitic model on \\(Y\\) by factorizing it. This additional relation enables feature labelling and increase prediction capabilities when similar rare words share similar labels",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "correctcmf",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<h3>Symmetric Block Matrix</h3>\n<ul>\n<li>Document - Word matrix \\(Y^{(1)} \\in ( \\Re \\cup \\lbrace ? \\rbrace )^{N\\times F}\\)\n</li>\n<li>Document - Label matrix  \\(Y^{(2)} \\in \\lbrace 0,1,? \\rbrace ^{N\\times K}\\)\n</li>\n<li>\nWord - Label matrix \\( Y^{(3)} \\in \\lbrace 0, 1 ,? \\rbrace ^{F\\times K} \\)\n</li>\n</ul>\n\nA \\((n + F + K) \\times (n + F + K) \\) symmetric block-matrix:\n\n$$\nY := \n\\left[\\begin{array}{ccc} \n[?]_{F\\times F} & Y^{(3)T} &Y^{(1)T}\n\\\\\nY^{(3)} & [?]_{K\\times K} & Y^{(2)T}\n\\\\\nY^{(1)} & Y^{(2)} & [?]_{N\\times N}\n\\end{array}\\right] \n$$\n\nLet factorize it!",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "CMFformula",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Collective Matrix Factorization\n\nWe have \\\\(T\\\\) types of entity with embeddings \\\\(\\mathbf{U} = (\\mathbf{U}^{(1)}, \\cdots, \\mathbf{U}^{(T)})\\\\) and we observe relations between all possible entity types \\\\(\\mathbf{Y} = (\\mathbf{Y}^{(1,1)}, \\mathbf{Y}^{(1,2)}, \\cdots, \\mathbf{Y}^{(T,T)}) \\\\)\n\n\nGeneral Loss functions \\\\(\\ell_{tt'}\\\\):\n\n\\\\(\\mathcal{L}(\\mathbf{U}) = \\sum_{t=1}^T \\sum_{t < t'< T} \\ell_{tt'}(\\mathbf{U}^{(t)}\\mathbf{U}^{(t')T}; \\mathbf{Y}^{(t,t')}||) \\\\)\n\nFor quadratic loss:\n\n\\\\(\\mathcal{L}(\\mathbf{U}) = \\sum_{t=1}^T \\sum_{t < t'< T} \\frac{1}{\\sigma_{tt'}^2} ||\\mathbf{U}^{(t)}\\mathbf{U}^{(t')T} - \\mathbf{Y}^{(t,t')}||_F^2 \\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "generalization",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Generalization to Arbitrary Databases\n\n<br />\n<img src=\"../../assets/figures/06/cmf_databases.png\" width=\"95%\">",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "example_denoising_faces",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Example: Augmented Multi-View\n*Objective*: Predict one view given the other\n\nPixel similarity should help. We create a binary matrix linking pixels.\n\n- Relation 1: pixel intensity value in the first image (real)\n- Relation 2: pixel intensity value in the second image (real)\n- Relation 3: Relative similarity between pixels (binary)\n\n<br />\n<img src=\"../../assets/figures/06/faces.png\" width=\"45%\">\n<img src=\"../../assets/figures/06/faces_expt.png\" width=\"45%\">\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "BayesianCMF",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Bayesian Modelling\nCMF: many regularization parameters.\n\nTuning is painful\n\nBayesian learning: principled automatic tuning\n\n1. Choice of a prior \\\\(P(\\mathbf{U},\\mathbf{V})\\\\)\n2. Find (approximate!) the posterior \\\\(P(\\mathbf{Y}|\\mathbf{U},\\mathbf{V})\\\\)\n\nAlgorithms:\n- Sampling\n    - Gibbs sampling: Salakhutdinov et al., 2008\n    - HMC: Mohamed et al., 2009\n- Variational Inference\n    - Variational Bayes: Raiko et al., 2008          \n    - Expectation Propagation: Stern et al., 2009",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "BayesianMatrixFactorization",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Bayesian Matrix Factorization\n\nIndependent Gaussian Priors: \\\\(P(\\mathbf{U},\\mathbf{V}) \\propto e^{-\\lambda (||\\mathbf{U}||_F^2 + ||\\mathbf{V}||_F^2)} \\\\)\n\nHomoscedastic Gaussian likelihood: \\\\(P(\\mathbf{Y}|\\mathbf{U},\\mathbf{V}) \\propto e^{-\\frac{1}{\\sigma^2} (||\\mathbf{U}\\mathbf{V}^T - \\mathbf{Y}||_F^2)} \\\\)\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "algobmf",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Algorithms for Bayesian Matrix Factorization\n\n*Block Gibbs sampling* \n\n- Sample \\\\(\\mathbf{u_i}|\\mathbf{V}, \\mathbf{Y}_{i:}\\\\) for \\\\(i=1,\\cdots,N\\\\)\n- Sample \\\\(\\mathbf{v_j}|\\mathbf{U}, \\mathbf{Y}_{:j}\\\\) for \\\\(j=1,\\cdots,M\\\\)\n\n*Variational Bayes* : Fully factorized approximation: \\\\( Q(\\mathbf{U},\\mathbf{V}) = \\prod_{i=1}^N Q(\\mathbf{u_i}) \\prod_{j=1}^M Q(\\mathbf{v_j}) \\\\)\n\n- Compute  \\\\(Q(\\mathbf{u}_i)\\\\) based on \\\\(\\mathbf{V}\\\\) and \\\\(\\mathbf{Y}_{i:}\\\\)\n- Compute  \\\\(Q(\\mathbf{v}_j)\\\\) based on \\\\(\\mathbf{U}\\\\) and \\\\(\\mathbf{Y}_{:j}\\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "BaysesianCMFdetails",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Bayesian Collective Matrix Factorization\n\nWe have \\\\(T\\\\) types of entity with embeddings \\\\(\\mathbf{U} = (\\mathbf{U}^{(1)}, \\cdots, \\mathbf{U}^{(T)})\\\\) and we observe relations between all possible entity types \\\\(\\mathbf{Y} = (\\mathbf{Y}^{(1,1)}, \\mathbf{Y}^{(1,2)}, \\cdots, \\mathbf{Y}^{(T,T)}) \\\\)\n\nIndependent Gaussian Priors: \\\\(P(\\mathbf{U}) \\propto e^{-\\sum_{t=1}^T \\lambda_t (||\\mathbf{U}^{(t)}||_F^2} \\\\)\n\nPer-relation Homoscedastic Gaussian likelihood: \\\\(P(\\mathbf{Y}|\\mathbf{U}) \\propto \\prod_{t=1}^T \\prod_{t < t'< T} e^{-\\frac{1}{\\sigma_{tt'}^2} (||\\mathbf{U}^{(t)}\\mathbf{U}^{(t')T} - \\mathbf{Y}^{(t,t')}||_F^2)} \\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "bcmf",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Algorithms for Bayesian CMF\n\n*Block Gibbs sampling*\n\n- For each type \\\\(t=1,\\cdots,T\\\\)\n    - Sample \\\\(\\mathbf{u^{(t)}_i}|\\mathbf{U^{(-t)}}, \\mathbf{Y}\\\\) for \\\\(i=1,\\cdots,N_t\\\\)\n\n*Variational Bayes* : Fully factorized approximation: \\\\( Q(\\mathbf{U}) = \\prod_{t=1}^T \\prod_{i=1}^{N_t} Q(\\mathbf{u}_i^{(t)})  \\\\)\n\n- Compute  \\\\(Q(\\mathbf{u}_i^{(t)})\\\\) based on \\\\(\\mathbf{U}^{(-t)}\\\\) and \\\\(\\mathbf{Y}\\\\)\n-\n\nNow we can maximize the likelihood with respect to \\\\(\\sigma_{tt'}\\\\) and \\\\(\\lambda_t\\\\) with \\\\(t,t'\\\\) \n",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "genes",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Gene Expression Data Experiment\n\n40 patients with breast cancer\n\n* *Views*: 2 measurements of 4287 genes.\n\n* *Task*: predicting random missing entries\n\n<br />\n<img src=\"../../assets/figures/06/gene_expression.png\" width=\"500\">\n\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "conclusion",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Conclusion to Collective Matrix Factorization\n\n* Collective Matrix Factorization and Tensor Factorization: \n    * Learn more by fusing multiple databases\n    * flexible and generic model for relational data\n    * Bayesian learning : automatic tuning of parameters and complexity\n\n\n* Collective Matrix Factorization vs. Tensor Factorization: \n    * CMF is easier because it deals only with matrices\n    * CMF = typed data. Tensor = multi-relational   \n    * Augmented multi-view: common setup\n\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
