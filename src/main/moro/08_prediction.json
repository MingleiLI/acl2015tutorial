{
  "name" : "Discriminative Factorial Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "IntroToDiscriminativeFactorialModels",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Discriminative Factorial Models\n\nFactorization is an efficient way of reducing the effective number of parameters in discriminative/predictive models\n\nWe introduce:\n\n* Factorization Machines\n* Multi-Task/Multi-Label Learning with Matrix Factorization\n* Structured Prediction with Factorized Parameters",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "fm",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Factorization Machines\n\nBinary Matrix Factorization as a Logistic Regression\nMatrix \\\\( Y\\in\\Re^{n,d} \\\\). For any \\\\(K\\\\):\n\n\\\\( P(\\mathbf{Y}_{ij}=1|i,j) = \\sigma(\\langle \\mathbf{U}_{i:}, \\mathbf{V}_{j:}\\rangle) \\\\) where \\\\(\\mathbf{U}\\in\\Re^{N\\times K}\\\\) and \\\\(\\mathbf{V}\\in\\Re^{M\\times K}\\\\)\n\nIs equivalent to:\n\n\\\\( P(y_{t}=1|x_t = (i,j) ) = \\sigma(\\langle\\mathbf{\\Theta}, \\Phi(x_t)\\rangle) \\\\) \n\nu.c. rank(\\\\(\\mathbf{\\Theta}\\\\)) = *K*\n\n- \\\\(\\Phi(x_t) = \\mathbf{e}_i \\otimes \\mathbf{e}_j\\\\) : standard MF\n- \\\\(\\Phi(x_t) = [1;\\mathbf{e}_i] \\otimes [1;\\mathbf{e}_j]\\\\) : MF with row and columns bias\n\n\\\\( \\rightarrow \\\\) called **factorization machine** [Rendle 2010]\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "rrr",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Reduced Rank Regression\n\nMultiple Linear Regression:\n\n- *d* Inputs: \\\\(\\mathbf{X}\\in\\Re^{n \\times d}\\\\)\n- *K* Outputs: Matrix \\\\( \\mathbf{Y}\\in\\Re^{n \\times K} \\\\)\n\nLinear regression:\n\n\n\\\\( \\mathrm{min}\\ \\sum_{k=1}^K \\sum_{i=1}^n (y_{ik} - x_{i:}^T \\theta_k)^2  = ||\\mathbf{Y}-\\mathbf{X} \\mathbf{\\Theta} ||^2_F\\\\)\n\n- Parameters: Matrix \\\\( \\mathbf{Y}\\in\\Re^{n \\times K} \\\\)\n\n\n**Reduced Rank Regression** assumes \\\\( \\mathbf{\\Theta} = \\mathbf{U} \\mathbf{V}^T \\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "multitask",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Multi-Task Learning\n* Inputs: \\\\(d\\\\) feature \\\\(\\mathbf\\phi(x_i)\\in\\Re^d\\\\)\n* Outputs: \\\\(K\\\\) labels \\\\(\\mathbf{y}_i\\in\\lbrace 0,1 \\rbrace^K\\\\)\n\n\\\\( P(\\mathbf{y}_{i}|x_i) = \\prod_{k=1}^K \\mathbf{\\sigma}(\\mathbf\\phi(x_i) \\mathbf{U} \\mathbf{v}_{k:}^T) \\\\)\n\nCan be interpreted as label embedding: \\\\(\\mathbf{v}_{k:}\\\\) is the \\\\(R\\\\)-dimensional embedding of the \\\\(k\\\\)-th label.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "SP",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Structured Prediction\n\nGoal: Predict \\\\(y^* = \\arg\\max_{y\\in T(\\mathbf{x})} S(\\mathbf{x},\\mathbf{y};\\mathbf{\\Theta})\\\\)\n\n- \\\\(\\mathbf{y}\\\\) is a structured output\n\n- \\\\(S(\\mathbf{x},\\mathbf{y};\\mathbf{\\Theta}) = <\\mathbf{\\Phi}(\\mathbf{x},\\mathbf{y}), \\mathbf{\\Theta}>\\\\) is a score function\n\n- \\\\(T(\\mathbf{x})\\\\) is all possible structures\n\nIn NLP \\\\(\\mathbf{\\Theta}\\\\) is often a vector, but it can be a matrix or tensor",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "SPex",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Examples of NLP Structured Prediction\n\n<br />\n<img src=\"../../assets/figures/08/s8s2.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s3.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex2",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s4.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex3",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Difficulties Selection\n####Few Templates:\n\n- Poor Performance\nMany Templates:\n\n- High Performance, Many Parameters\n- Interactions between features can be hard to know\n\n####Alternatives? Automatic Feature Selection:\n\n- Effective, but ''messy'' (Zhao, 2009)\n- Expensive\n**Solution**: Learn templates from data using matrix factorization",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "pstep1",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Step 1: Identify Simple Features\n\nFeature vectors for each type of information: \n\n - \\\\(\\phi_{head}\\\\), vector \\\\(\\in \\Re^n\\\\) for head token\n - \\\\(\\phi_{child}\\\\), vector \\\\(\\in \\Re^n\\\\)for child token\n - \\\\(\\phi_{arc}\\\\), vector \\\\(\\in \\Re^n\\\\) for arc information\n\n### Step 2: Define Composite Features\n\n - \\\\(\\phi_{head} \\otimes \\phi_{child}\\\\), matrix \\\\(\\in \\Re^{n\\times n}\\\\) \n - \\\\(\\phi_{arc} \\otimes \\phi_{child}\\\\), matrix \\\\(\\in \\Re^{n\\times n}\\\\) \n - \\\\(\\phi_{head} \\otimes \\phi_{arc}\\\\), matrix \\\\(\\in \\Re^{n\\times n}\\\\) \n - \\\\(\\phi_{head} \\otimes \\phi_{child} \\otimes \\phi_{arc}\\\\), matrix \\\\(\\in \\Re^{n\\times n \\times n}\\\\) \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "pstep2",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Step 2: Formulate Model Parameters Tensor\n\nA tensor \\\\(\\Re^{n \\times n \\times d}\\\\), describes the concatenation of all three feature vectors, so replace \\\\(\\Theta\\\\) with tensor \\\\(A\\\\):\n\n $$ S(\\mathbf{x},\\mathbf{y};\\mathbf{\\Theta})  <\\phi_{head} \\otimes \\phi_{child} \\otimes \\phi_{arc},\\mathbf{\\Theta}> $$ \n\n Can be huge!  A better option?  Low rank approximation.  Calculate A as: \n\n $$ \\mathbf{\\Theta} = \\sum_i U_i \\otimes V_i \\otimes W_i $$ \n\nwhere \\\\(U,V,\\\\) and \\\\(W\\\\) are dense low-dimensional representations \\\\(\\in \\Re^{n\\times r}\\\\) ",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "tobjectives1",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Learning\n\n Training Objective: \n\n $$ C \\sum_i \\eta_i + ||U||_F^2 + ||V||_F^2 + ||W||_F^2 + ||A||_F^2 $$ \n\nNon-convex optimization with regularization\n\nCan optimize with variant of Passive Aggressive (Crammer, 2006) \n",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "tobjectives2",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Learning\n\nOnline method tailored for tensors [Lei et al., 2014]\n\nIterate over data and for each instance, update \\\\(A\\\\) and choose one of the feature tensors as follows: \n\n - \\\\( A^{(t + 1)} = A^t + \\Delta A \\\\) \n\n - \\\\( U^{(t+1)} = U^t + \\Delta U \\\\) \n\n With sub-problem (solvable with closed-form solution): \n\n $$ \\min_{\\Delta A, \\Delta U} \\frac{1}{2} ||\\Delta A ||_F^2 + \\frac{1}{2} ||\\Delta U ||_F^2 + C \\eta_i $$ ",
      "extraFields" : { }
    }
  }, {
    "id" : 30,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "parseresults",
      "extraFields" : { }
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Performance - Parsing\n\n<img src=\"../../assets/figures/08/parser-results.png\" height=\"400\">\n\n- RBG + MF model trained with rank \\\\(r = 50\\\\) tensors\n\n- Results averaged over 14 languages (CoNLL data)",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "srlresults",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Performance - SRL\n\n<img src=\"../../assets/figures/08/srl-results.png\" height=\"400\">\n\n - Baselines have many features and automatic feature selection\n\n - Outperforms previous best system across many languages",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "conclusion",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Conclusion \n\n* **Discriminative** models can be parameterized with matrices or tensors\n* **Low-rank** asumption enables **parameter sharing**\n    * **Multi-task** learning: corresponds to label-embedding\n    * **Vanilla matrix factorization** can be expressed as a \\\\(N\\times M\\\\)-dimensional **logistic regression with rank constraint**\n    * **structured prediction** with low rank parameters can improve performance over heavily engineered state-of-the-art systems\n\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}