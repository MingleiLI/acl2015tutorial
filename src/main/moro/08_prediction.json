{
  "name" : "Discriminative Factorial Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "IntroToDiscriminativeFactorialModels",
      "extraFields" : {
        "reveal_params" : "data-background: #00ffff"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Discriminative Factorials Models\n\nFactorization is an efficient way of reducing the effective number of parameters in discriminative/predictive models\n\nWe introduce:\n\n* Multi-Task/Multi-Label Learning with Matrix Factorization\n* Factorization Machines\n* Structured Prediction with Factorized Parameters",
      "extraFields" : {
        "reveal_params" : "data-background: #00ffff"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "MultiTask",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Multi-Task Learning\n* Inputs: \\\\(d\\\\) feature \\\\(\\mathbf\\phi(x_i)\\in\\Re^d\\\\)\n* Outputs: \\\\(K\\\\) labels \\\\(\\mathbf{y}_i\\in\\lbrace 0,1 \\rbrace^K\\\\)\n\n\\\\( P(\\mathbf{y}_{i}|x_i) = \\prod_{k=1}^K \\mathbf{\\sigma}(\\mathbf\\phi(x_i) \\mathbf{U} \\mathbf{V}_{k:}^T) \\\\)\n\nCan be interpreted as label embedding: \\\\(\\mathbf{V}_{k:}\\\\) is the \\\\(R\\\\)-dimensional embedding of the \\\\(k\\\\)-th label.",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "MatrixFactorizationAsLinReg",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Matrix Factorization as a Constrained Linear Regression\nMatrix \\\\( \\mathbf{Y}\\in\\Re^{n \\times d} \\\\). For any \\\\(K\\\\):\n\n\\\\( \\mathbb{E}[y_{ij}=1|i,j] = \\langle \\mathbf{U}_{i:}, \\mathbf{V}_{j:}\\rangle \\\\) where \\\\(\\mathbf{U}\\in\\Re^{N\\times K}\\\\) and \\\\(\\mathbf{V}\\in\\Re^{M\\times K}\\\\)\n\nEquivalently, we consider the prediction of the matrix values \\\\(y_{ij}\\\\) given the row and column indices:\n\n\\\\( P(y_{t}=1|x_t = (i,j) ) = \\sigma(\\langle\\mathbf{\\Theta}, \\mathbf{e}_i \\otimes \\mathbf{e}_j\\rangle) \\\\) \n\nwhere \\\\( \\mathbf{\\Theta} = \\mathbf{U} \\mathbf{V}^T \\\\)\n\nSo we can view Matrix Factorization as a Rank-Constrained Linear Regression\n\n==> called **factorization machine** [Rendle 2010]",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "BinMFAsLogReg",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Binary Matrix Factorization as a Logistic Regression\nMatrix \\\\( Y\\in\\Re^{n,d} \\\\). For any \\\\(K\\\\):\n\n\\\\( P(\\mathbf{Y}_{ij}=1|i,j) = \\sigma(\\langle \\mathbf{U}_{i:}, \\mathbf{V}_{j:}\\rangle) \\\\) where \\\\(\\mathbf{U}\\in\\Re^{N\\times K}\\\\) and \\\\(\\mathbf{V}\\in\\Re^{M\\times K}\\\\)\n\nIs equivalent to:\n\n\\\\( P(y_{t}=1|x_t = (i,j) ) = \\sigma(\\langle\\mathbf{\\Theta}, \\mathbf{e}_i \\otimes \\mathbf{e}_j\\rangle) \\\\) ",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "SP",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Structured Prediction\n\nGoal: Predict \\\\(y^* = \\arg\\max_{y\\in T(x)} S(x,y;\\Theta)\\\\)\n\n- y is a structured output\n\n- \\\\(S(x,y;\\Theta)\\\\) is a score function\n\n- \\\\(T(x)\\\\) is all possible structures\n\nIn NLP \\\\(\\Theta\\\\) is often a vector, but it can be a matrix or tensor",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "SPex",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Examples of Matrix/Tensor Factorization\n\n<br />\n<img src=\"../../assets/figures/08/s8s2.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s3.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex2",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s4.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "concatex3",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s5.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "huh",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br />\n<img src=\"../../assets/figures/08/s8s6.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "manualdownsides",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Difficulties Selection\nFew Templates:\n\n- Poor Performance\n \nMany Templates:\n\n- High Performance, Many Parameters\n\n- Interactions between features can be hard to know\n\nAlternatives? Automatic Feature Selection:\n\n- Effective, but ''messy'' (Zhao, 2009)\n\n- Expensive\n\n**Solution**: Learn templates from data using matrix factorization",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "pstep1",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Step 1: Formulate Feature Vector as Rank 1 Tensor\n\n Feature vectors for each type of information: \n\n - \\\\(\\phi_{head}\\\\), vector \\\\(\\in R^n\\\\) for head token\n\n - \\\\(\\phi_{child}\\\\), vector \\\\(\\in R^n\\\\)for child token\n\n - \\\\(\\phi_{arc}\\\\), vector \\\\(\\in R^d\\\\) for arc information",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "pstep2",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Step 2: Formulate Model Parameters Tensor\n\n - A tensor \\\\(R^{n \\times n \\times d}\\\\), describes the concatenation of all three feature vectors, so replace \\\\(\\Theta\\\\) with tensor \\\\(A\\\\):\n\n $$ S(x,y;\\Theta) = <\\phi_{head} \\otimes \\phi_{child}> $$ \n\n Can be huge!  A better option?  Low rank approximation.  Calculate A as: \n\n $$ A = \\sum_i U_i \\otimes V_i \\otimes W_i $$ \n\n Where:\n\n - r is the rank\n\n - \\\\(U, V \\in R^{r \\times n}\\\\) replace \\\\(\\phi_{head}\\\\), \\\\(\\phi_{child}\\\\) \n\n - \\\\(W \\in R^{r\\times d} \\\\) \n\n - \\\\(U,V,\\\\) and \\\\(W\\\\) are dense low-dimensional representations \\\\(\\in R^r\\\\) ",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "tobjectives1",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Learning\n\n Training Objective: \n\n $$ C \\sum_i \\eta_i + ||U||_F^2 + ||V||_F^2 + ||W||_F^2 + ||\\Theta||_F^2 $$ \n\nNon-convex optimization \n\nCan optimize with variant of Passive Aggressive (Crammer, 2006) \n",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "tobjectives2",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Learning\n\nOnline method tailored for tensors (Lei, 2014)\n\nIterate over data and for each instance, update \\\\(\\Theta\\\\) and choose one of the feature tensors as follows: \n\n - \\\\( \\Theta^{(t + 1)} = \\Theta^t + \\Delta \\Theta \\\\) \n\n - \\\\( U^{(t+1)} = U^t + \\Delta U \\\\) \n\n With sub-problem (solvable with closed-form solution): \n\n $$ \\min_{\\Delta \\Theta, \\Delta U} \\frac{1}{2} ||\\Delta \\Theta ||_F^2 + \\frac{1}{2} ||\\Delta \\Theta ||_F^2 + C \\eta_i $$ ",
      "extraFields" : { }
    }
  }, {
    "id" : 30,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "parseresults",
      "extraFields" : { }
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Performance\n\n<br />\n<img src=\"../../assets/figures/08/parse_results.png\" height=\"300\">\n\n - Great Parser #1: Turbo Parser (Martins, 2013)\n \n - Great Parser #2: Minimum Spanning Tree (McDonald, 2005)\n\n - Rank 50 tensors\n\n - Results averaged over 14 languages (CoNLL data)",
      "extraFields" : { }
    }
  }, {
    "id" : 32,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "conclusion",
      "extraFields" : { }
    }
  }, {
    "id" : 33,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Conclusion \n\n* **Discriminative** models can be parameterized with matrices or tensors\n* **Low-rank** asumption enables **parameter sharing**\n    * **Multi-task** learning: corresponds to label-embedding\n    * **Vanilla matrix factorization** can be expressed as a \\\\(N\\times M\\\\)-dimensional **logistic regression with rank constraint**\n    * **structured prediction** with low rank parameters has a lot of potential\n\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
