{
  "name" : "Collective Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "MotivatingExample",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Motivating Example\nAssume we have 2 relations:\n\n1. Document - Word matrix \\\\( Y^{(1)} \\in ( \\Re \\cup \\lbrace ? \\rbrace )^{n\\times F} \\\\)\n2. Document - Label matrix  \\\\( Y^{(2)} \\in \\lbrace 0,1,? \\rbrace ^{n\\times K} \\\\)\n\nRepresenting is a a 2-slices tensor is overly complicated and not needed.\nA simple and effective way to represent it is by concatenating the two matrices into a big \\\\( n\\times (F+K) \\\\) matrix:\n\n\\\\( Y := [ Y^{(1)} \\ Y^{(2)}]  \\\\)\n\nWe can estimate a probabilitic model on \\\\(Y\\\\) by factorizing it.\n\nJust use the binary matrix factorization technique we saw before. This is a new method for supervised classification!\n\n - Handles missing data in the input\n - Can work with partially labelled documents\n - Takes advantage of unlabelled data\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "MotivatingExampleContinued",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Motivating Example\nWe have more information. We want to add a relation corresponding to labelled features:\n\n3. Word - Label matrix \\\\( Y^{(3)} \\in \\lbrace 0, 1 ,? \\rbrace ^{F\\times K} \\\\)\n\nBy concatenation, we have a \\\\( (n + F) \\times (F+K) \\\\) matrix:\n\n\\\\( Y := \\begin{array}{cc} \nY^{(1)} & Y^{(2)} \\\\\nY^{(3)} & [?]_{F\\times F} \n\\end{array}  \\\\)\n\nWe can estimate a probabilitic model on \\\\(Y\\\\) by factorizing it. This additional relation enables feature labelling and increase prediction capabilities when similar rare words share similar labels.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "MotivatingExampleFinal",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Motivating Example\nSomething is missing: the labels of the words and of the documents are the same, but the model does not use this information.\n\nWe will create a \\\\( (n + F + K) \\times (n + F + K) \\\\) symmetric matrix:\n\n\\\\( Y := \\begin{array}{cc} \n[?]_{n\\times n} Y^{(1)} & Y^{(2)} \\\\\nY^{(1)T} & [?]_{F\\times F} & Y^{(3)T} \\\\ \nY^{(2)T} & Y^{(3)} & [?]_{K\\times K}\n\\end{array}  \\\\)\n\nWe can estimate a probabilitic model on \\\\(Y\\\\) by factorizing it. This duplication of data enables the factorization to use the data from both document and word in the label embeddings.\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
